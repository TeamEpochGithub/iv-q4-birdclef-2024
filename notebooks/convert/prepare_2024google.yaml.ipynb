{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-17T08:14:38.444523Z",
     "start_time": "2024-05-17T08:14:38.434172Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "only_silences = True\n",
    "\n",
    "AUDIO_PATH = Path(\"../data/raw/2024/unlabeled_soundscapes\")\n",
    "FILE_PATHS = list(AUDIO_PATH.glob(\"*.ogg\"))\n",
    "PREDICTIONS_OUTPUT_PATH = Path(\"../data/raw/2024/google-bvc-predictions.csv\")\n",
    "SAMPLE_RATE = 32000\n",
    "OUTPUT_PATH = Path(\"../data/raw/2024gsil/train_audio/\")\n",
    "META_PATH = Path(\"../data/raw/2024gsil/train_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Load in preds\n",
    "\n",
    "df_preds = pd.read_csv(PREDICTIONS_OUTPUT_PATH)\n",
    "#Get the rows that \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T08:14:41.849576Z",
     "start_time": "2024-05-17T08:14:38.445139Z"
    }
   },
   "id": "8dea59d35af1f0ce",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_108521/3316257484.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sliced[\"file\"] = sliced[\"row_id\"].apply(lambda x: x.split(\"_\")[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03788649511270696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 1957/1957 [04:50<00:00,  6.74it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[38], line 48\u001B[0m\n\u001B[1;32m     45\u001B[0m             train_metadata[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprimary_label\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msilent\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     47\u001B[0m     \u001B[38;5;66;03m#Save metatdata to disk\u001B[39;00m\n\u001B[0;32m---> 48\u001B[0m     metadata_df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_metadata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     metadata_df\u001B[38;5;241m.\u001B[39mto_csv(META_PATH, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/Documents/Epoch/q4-bird-clef/venv/lib/python3.10/site-packages/pandas/core/frame.py:778\u001B[0m, in \u001B[0;36mDataFrame.__init__\u001B[0;34m(self, data, index, columns, dtype, copy)\u001B[0m\n\u001B[1;32m    772\u001B[0m     mgr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_mgr(\n\u001B[1;32m    773\u001B[0m         data, axes\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindex\u001B[39m\u001B[38;5;124m\"\u001B[39m: index, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumns\u001B[39m\u001B[38;5;124m\"\u001B[39m: columns}, dtype\u001B[38;5;241m=\u001B[39mdtype, copy\u001B[38;5;241m=\u001B[39mcopy\n\u001B[1;32m    774\u001B[0m     )\n\u001B[1;32m    776\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, \u001B[38;5;28mdict\u001B[39m):\n\u001B[1;32m    777\u001B[0m     \u001B[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001B[39;00m\n\u001B[0;32m--> 778\u001B[0m     mgr \u001B[38;5;241m=\u001B[39m \u001B[43mdict_to_mgr\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtyp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmanager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    779\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, ma\u001B[38;5;241m.\u001B[39mMaskedArray):\n\u001B[1;32m    780\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mma\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m mrecords\n",
      "File \u001B[0;32m~/Documents/Epoch/q4-bird-clef/venv/lib/python3.10/site-packages/pandas/core/internals/construction.py:503\u001B[0m, in \u001B[0;36mdict_to_mgr\u001B[0;34m(data, index, columns, dtype, typ, copy)\u001B[0m\n\u001B[1;32m    499\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    500\u001B[0m         \u001B[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001B[39;00m\n\u001B[1;32m    501\u001B[0m         arrays \u001B[38;5;241m=\u001B[39m [x\u001B[38;5;241m.\u001B[39mcopy() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(x, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m x \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m arrays]\n\u001B[0;32m--> 503\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43marrays_to_mgr\u001B[49m\u001B[43m(\u001B[49m\u001B[43marrays\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtyp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtyp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconsolidate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Epoch/q4-bird-clef/venv/lib/python3.10/site-packages/pandas/core/internals/construction.py:114\u001B[0m, in \u001B[0;36marrays_to_mgr\u001B[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001B[0m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m verify_integrity:\n\u001B[1;32m    112\u001B[0m     \u001B[38;5;66;03m# figure out the index, if necessary\u001B[39;00m\n\u001B[1;32m    113\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m index \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 114\u001B[0m         index \u001B[38;5;241m=\u001B[39m \u001B[43m_extract_index\u001B[49m\u001B[43m(\u001B[49m\u001B[43marrays\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    116\u001B[0m         index \u001B[38;5;241m=\u001B[39m ensure_index(index)\n",
      "File \u001B[0;32m~/Documents/Epoch/q4-bird-clef/venv/lib/python3.10/site-packages/pandas/core/internals/construction.py:677\u001B[0m, in \u001B[0;36m_extract_index\u001B[0;34m(data)\u001B[0m\n\u001B[1;32m    675\u001B[0m lengths \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mset\u001B[39m(raw_lengths))\n\u001B[1;32m    676\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(lengths) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m--> 677\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAll arrays must be of the same length\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    679\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m have_dicts:\n\u001B[1;32m    680\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    681\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    682\u001B[0m     )\n",
      "\u001B[0;31mValueError\u001B[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "train_metadata = {\"filename\": [], \"labels\": [], \"rating\": [], \"primary_label\": [], \"secondary_labels\": []}\n",
    "\n",
    "\n",
    "if only_silences:\n",
    "    # Get the rows where the sum of the classes is <= 0.1\n",
    "    sliced = df_preds[df_preds.iloc[:,:182].sum(axis=1) <= 0.004]\n",
    "    print(len(sliced) / len(df_preds))\n",
    "    sliced[\"file\"] = sliced[\"row_id\"].apply(lambda x: x.split(\"_\")[0])\n",
    "    \n",
    "    # Group by the file and apply operations per group in loop\n",
    "    for group in tqdm(sliced.groupby(\"file\"), desc=\"Processing files\"):\n",
    "        file_name = group[0]\n",
    "\n",
    "\n",
    "        curr_df = group[1]\n",
    "        file = AUDIO_PATH /file_name\n",
    "        \n",
    "        segments = curr_df[\"row_id\"].apply(lambda x: int(x.split(\"_\")[1])).values\n",
    "        # print(segments)\n",
    "        \n",
    "        #Load the file using librosa\n",
    "        y, sr = librosa.load(file, sr=SAMPLE_RATE)\n",
    "        #Replace .ogg with empty\n",
    "        file_name = file_name.replace(\".ogg\", \"\")\n",
    "        #Slice the audio file into 5 second chunks\n",
    "        for segment in segments:\n",
    "            start = segment * SAMPLE_RATE\n",
    "            end = (segment + 5) * SAMPLE_RATE\n",
    "            \n",
    "            chunk = y[start:end]\n",
    "            \n",
    "            #Save the chunk to disk\n",
    "            output_file = OUTPUT_PATH / f\"{file_name}_{segment}.wav\"\n",
    "            sf.write(output_file, chunk, SAMPLE_RATE, format=\"WAV\")\n",
    "            \n",
    "\n",
    "            #Append the metadata to the metadata dict\n",
    "            train_metadata[\"filename\"].append(f\"{file_name}_{segment}.wav\")\n",
    "            train_metadata[\"labels\"].append([])\n",
    "            train_metadata[\"rating\"].append(5)\n",
    "            train_metadata[\"primary_label\"].append(\"silent\")\n",
    "        \n",
    "    #Save metatdata to disk\n",
    "    metadata_df = pd.DataFrame(train_metadata)\n",
    "    metadata_df.to_csv(META_PATH, index=False)\n",
    "else:\n",
    "    \n",
    "    unique_species = set()\n",
    "     # Get the rows where the sum of the classes is <= 0.1\n",
    "    sliced = df_preds[df_preds.iloc[:,:182].max(axis=1) >= 0.5]\n",
    "    print(len(sliced) / len(df_preds))\n",
    "    sliced[\"file\"] = sliced[\"row_id\"].apply(lambda x: x.split(\"_\")[0])\n",
    "    \n",
    "    # Group by the file and apply operations per group in loop\n",
    "    for group in tqdm(sliced.groupby(\"file\"), desc=\"Processing files\"):\n",
    "        file_name = group[0]\n",
    "    \n",
    "    \n",
    "        curr_df = group[1]\n",
    "        file = AUDIO_PATH /file_name\n",
    "        \n",
    "        segments = curr_df[\"row_id\"].apply(lambda x: int(x.split(\"_\")[1])).values\n",
    "        # print(segments)\n",
    "        \n",
    "        #Load the file using librosa\n",
    "        y, sr = librosa.load(file, sr=SAMPLE_RATE)\n",
    "        #Replace .ogg with empty\n",
    "        file_name = file_name.replace(\".ogg\", \"\")\n",
    "        #Slice the audio file into 5 second chunks\n",
    "        for i, segment in enumerate(segments):\n",
    "            start = segment * SAMPLE_RATE\n",
    "            end = (segment + 5) * SAMPLE_RATE\n",
    "            \n",
    "            chunk = y[start:end]\n",
    "            \n",
    "            #Save the chunk to disk\n",
    "            output_file = OUTPUT_PATH / f\"{file_name}_{segment}.wav\"\n",
    "            sf.write(output_file, chunk, SAMPLE_RATE, format=\"WAV\")\n",
    "            \n",
    "            #Get the highest column in the current segment to obtain the primary label\n",
    "            primary_label = curr_df.iloc[i, :182].idxmax()\n",
    "            unique_species.add(primary_label)\n",
    "            \n",
    "            \n",
    "            # Get all the columns that are higher than 0.25\n",
    "            secondary_labels = curr_df.iloc[:, :182].columns[curr_df.iloc[i, :182] >= 0.2].tolist()\n",
    "            \n",
    "            #Remove the primary label from the secondary labels\n",
    "            secondary_labels.remove(primary_label)\n",
    "            \n",
    "            #Add all the secondary labels to the unique species set\n",
    "            unique_species.update(secondary_labels)\n",
    "            \n",
    "            #Append the metadata to the metadata dict\n",
    "            train_metadata[\"filename\"].append(f\"{file_name}_{segment}.wav\")\n",
    "            train_metadata[\"rating\"].append(5)\n",
    "            train_metadata[\"primary_label\"].append(primary_label)\n",
    "            train_metadata[\"secondary_labels\"].append(secondary_labels)\n",
    "            \n",
    "        print(f\"Lenght of unique species: {len(unique_species)}\")\n",
    "            \n",
    "        \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T08:19:32.341685Z",
     "start_time": "2024-05-17T08:14:41.850294Z"
    }
   },
   "id": "454ea8bb3a892313",
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if train_metadata[\"labels\"] == []:\n",
    "    del train_metadata[\"labels\"]\n",
    "\n",
    "if train_metadata[\"secondary_labels\"] == []:\n",
    "    del train_metadata[\"secondary_labels\"]\n",
    "#Save metatdata to disk\n",
    "metadata_df = pd.DataFrame(train_metadata)\n",
    "metadata_df.to_csv(META_PATH, index=False)   \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T08:19:54.407326Z",
     "start_time": "2024-05-17T08:19:54.392142Z"
    }
   },
   "id": "376dada5c29459e1",
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f0797abd23215d8e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
