defaults:
  - pipeline/default@_here_
  - _self_

x_sys:
  steps:
    - _target_: src.modules.transformation.x.nan_to_zero.NanToZero
      years: ["2024"]
y_sys:

train_sys:
  steps:
    - _target_: src.modules.training.main_trainer.MainTrainer
      year: "2024"
      dataset_args:
        sampler:
          _target_: src.modules.training.datasets.sampler.random.Random
          length: 160000
        to_2d:
          _target_: src.modules.training.datasets.to_2d.spec.Spec
          spec:
           _target_: functools.partial
           _args_:
             - _target_: hydra.utils.get_class
               path: torchaudio.transforms.MelSpectrogram #nnAudio.features.mel.MelSpectrogram
          output_shape: [256, 256]
          scale:
            _target_: src.modules.training.datasets.to_2d.spec_normalize.SpecNormalize

        #  _target_: src.modules.training.datasets.to_2d.reshape.Reshape
        #  shape: [400,400]
        labeler:
          _target_: src.modules.training.datasets.labeler.psd_labeler.PSDLabeler
        filter_:
          _target_: src.modules.training.datasets.filter.grade_threshold.GradeThreshold
          threshold: 0
        aug_1d:
          _target_: epochalyst.pipeline.model.training.augmentation.utils.CustomSequential
          x_transforms:
            - _target_: epochalyst.pipeline.model.training.augmentation.time_series_augmentations.RandomPhaseShift
              p: 0.5
              shift_limit: 0.5
            - _target_: epochalyst.pipeline.model.training.augmentation.time_series_augmentations.RandomAmplitudeShift
              p: 0.5
          xy_transforms:
            - _target_: epochalyst.pipeline.model.training.augmentation.time_series_augmentations.CutMix1D
              p: 0.5
            - _target_: epochalyst.pipeline.model.training.augmentation.time_series_augmentations.MixUp1D
              p: 0.5
        aug_2d:
          _target_: epochalyst.pipeline.model.training.augmentation.utils.CustomSequential
          x_transforms:
            - _target_: kornia.augmentation.RandomErasing
              p: 0.25
            - _target_: kornia.augmentation.auto.RandAugment
              n: 2
              m: 9
              # this is the default policy, excluding problematic augmentations that crash (for 1 channel img)
              policy:
                #                - [[auto_contrast, 0, 1]]
                - [ [ equalize, 0, 1 ] ]
                #                - [[invert, 0, 1]] # simply does not make sense for spectrogram
                - [ [ rotate, -30.0, 30.0 ] ]
                - [ [ posterize, 0, 4 ] ]
                - [ [ solarize, 0, 1 ] ]
                - [ [ solarize_add, 0, 0.43 ] ]
                #                - [color, 0.1, 1.9]
                - [ [ contrast, 0.1, 1.9 ] ]
                - [ [ brightness, 0.1, 1.9 ] ]
                - [ [ sharpness, 0.1, 1.9 ] ]
                - [ [ shear_x, -0.3, 0.3 ] ]
                - [ [ shear_y, -0.3, 0.3 ] ]
                - [ [ translate_x, -0.1, 0.1 ] ]
                - [ [ translate_y, -0.1, 0.1 ] ]
          xy_transforms:
            - _target_: epochalyst.pipeline.model.training.augmentation.image_augmentations.CutMix
              p: 1
            - _target_: epochalyst.pipeline.model.training.augmentation.image_augmentations.MixUp
              p: 0.8
      dataloader_args:
        num_workers: 16
        prefetch_factor: 2
        persistent_workers: false
      n_folds: 5 # 0 for train full,
      epochs: 100
      patience: 15
      batch_size: 64
      model:
        _target_: src.modules.training.models.timm.Timm #Ensure that activation is sigmoid for non Logits loss functions.
        in_channels: 1
        out_channels: 182
        model_name: "eca_nfnet_l0"
        activation: "sigmoid"
      scheduler:
        _target_: functools.partial
        _args_:
          - _target_: hydra.utils.get_class
            path: timm.scheduler.cosine_lr.CosineLRScheduler
        warmup_t: 2
        warmup_lr_init: 1e-5
        t_initial: 100
        cycle_limit: 1
      criterion:
        _target_: torch.nn.BCELoss
      optimizer:
        _target_: functools.partial
        _args_:
          - _target_: hydra.utils.get_class
            path: torch.optim.AdamW
        lr: 1e-4
